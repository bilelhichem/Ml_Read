```markdown
## Data Preprocessing en Python

### 1. Introduction

Le pr√©traitement des donn√©es est une √©tape essentielle en Data Science. Ce processus permet de nettoyer, transformer et pr√©parer les donn√©es pour une analyse plus efficace et pr√©cise.

### 2. √âtapes n√©cessaires

#### 2.1 Importation des biblioth√®ques

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

#### 2.2 Lecture du jeu de donn√©es

```python
df = pd.read_csv("./student/train.csv")
df.head()
```

#### 2.3 V√©rification de la coh√©rence des donn√©es

```python
df.isnull().sum()  # Nombre de valeurs manquantes
df.isnull().sum() / df.shape[0] * 100  # Pourcentage de valeurs manquantes
```

**Gestion des valeurs manquantes selon les mod√®les :**

- **XGBoost** : G√®re directement les valeurs manquantes. Il apprend le meilleur chemin pour les donn√©es manquantes lors de la construction des arbres.
- **Random Forest** : Ne supporte pas les valeurs manquantes directement. Il faut les imputer (moyenne, m√©diane...) avant d'entra√Æner le mod√®le.
- **Linear Regression** : Ne peut pas g√©rer les valeurs manquantes et n√©cessite une imputation pr√©alable.

#### 2.4 Gestion des valeurs manquantes et aberrantes

##### D√©tection des valeurs aberrantes avec un Boxplot

```python
sns.boxplot(x=df['colonne'])
```

#### M√©thodes de d√©tection des valeurs aberrantes

- **Intervalle interquartile (IQR)**

```python
Q1 = df['colonne'].quantile(0.25)
Q3 = df['colonne'].quantile(0.75)
IQR = Q3 - Q1
outliers = df[(df['colonne'] < (Q1 - 1.5 * IQR)) | (df['colonne'] > (Q3 + 1.5 * IQR))]
```

- **Z-score**

```python
from scipy import stats
df['z_score'] = np.abs(stats.zscore(df['colonne']))
outliers = df[df['z_score'] > 3]
```

#### 2.5 Conversion et v√©rification des types de donn√©es

```python
df.dtypes
df['colonne'] = df['colonne'].astype(int)  # Conversion en entier
```

#### 2.6 Analyse de la corr√©lation

```python
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
```

##### Mutual Information

La **mutual information** d√©tecte les relations complexes entre les variables et aide √† identifier les features les plus informatives pour la variable cible.

```python
from sklearn.feature_selection import mutual_info_regression
mi_scores = mutual_info_regression(df.drop('cible', axis=1), df['cible'])
pd.Series(mi_scores, index=df.drop('cible', axis=1).columns).sort_values(ascending=False)
```

#### 2.7 Encodage des variables cat√©gorielles

- **Label Encoding** : √† utiliser pour les variables ordinales (ordre logique entre les cat√©gories).
- **One Hot Encoding** : pr√©f√©rable pour les variables nominales, car chaque cat√©gorie devient une colonne binaire.

```python
df = pd.get_dummies(df, columns=['categorie'], drop_first=True)
```

#### 2.8 Normalisation et standardisation

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[['colonne']])
```

#### 2.9 Correction de la Skewness (asym√©trie)

La **skewness** permet de comprendre comment les donn√©es sont r√©parties et si des ajustements sont n√©cessaires.

- La transformation **Yeo-Johnson** aide √† rendre les donn√©es plus sym√©triques.

```python
from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method='yeo-johnson')
df['colonne_transforme'] = pt.fit_transform(df[['colonne']])
```

### 3. Manipulation des Donn√©es avec Pandas

#### 3.1 Cr√©ation et manipulation de DataFrame

```python
# Cr√©ation d'un DataFrame
df = pd.DataFrame({'Nom': ['Alice', 'Bob'], 'Age': [25, 30]})
df.columns = ['ID', 'Nom', 'Age']  # Renommer les colonnes
```

#### 3.2 Op√©rations courantes

```python
df.head()  # Afficher les premi√®res lignes
df.info()  # Informations sur le DataFrame
df.describe()  # Statistiques descriptives
df.dropna(inplace=True)  # Suppression des valeurs NaN
df.fillna(value=0, inplace=True)  # Remplacement des valeurs NaN
df.drop_duplicates(subset=['Nom'], keep='first', inplace=True)  # Suppression des doublons
df = pd.concat([df1, df2], ignore_index=True)  # Fusionner deux DataFrames
```

### 4. NumPy en Python

#### 4.1 Manipulation des tableaux NumPy

```python
import numpy as np
arr = np.array([1, 2, 3, 4, 5])  # Cr√©ation d'un tableau NumPy
arr = np.array([[1, 2, 3], [4, 5, 6]])  # Tableau 2D
arr[1:4]  # Extraction des √©l√©ments
np.random.shuffle(arr)  # M√©langer les valeurs
```

### 5. Large Language Models (LLMs)

#### 5.1 Niveaux d'utilisation des LLMs

- **Niveau 1 - Prompt Engineering** : R√©daction de prompts efficaces
- **Niveau 2 - Model Fine-tuning** : Adaptation d'un mod√®le pr√©-entra√Æn√©
- **Niveau 3 - Build Your Own** : Entra√Ænement d'un mod√®le √† partir de z√©ro
  
## Mod√®le

### 1) Fonction de co√ªt et erreur

La fonction que tu montres sur l‚Äôimage est la racine de l‚Äôerreur quadratique moyenne (RMSE - Root Mean Squared Error) :  
Utilis√©e pour √©valuer la performance du mod√®le apr√®s l‚Äôentra√Ænement.

```python
# Erreur quadratique moyenne (RMSE)
err = (1 / m) * sum((f_hat(x_i) - y_i) ** 2 for i in range(1, m + 1))
```

Alors que la fonction de co√ªt que tu avais mentionn√©e pr√©c√©demment est la fonction de co√ªt de la r√©gression lin√©aire (MSE - Mean Squared Error) :  
Utilis√©e pour entra√Æner le mod√®le et ajuster \(\theta_0, \theta_1\) en minimisant l‚Äôerreur.

```python
# Fonction de co√ªt (MSE)
J_theta = (1 / (2 * m)) * sum((h(x_i) - y_i) ** 2 for i in range(1, m + 1))
```

### 2) D√©composition de l'erreur biais-variance (RMSE)

```python
# Erreur totale
err = (1 / m) * sum((f_hat(x_i) - y_i) ** 2 for i in range(1, m + 1))
```

On d√©compose l'erreur en trois termes :  

- **Biais** (\(Bias^2(x_0)\))  
- **Variance** (\(Var[\hat{f}(x_0)]\))  
- **Bruit** (Erreur irr√©cup√©rable, qui provient du bruit dans les donn√©es)

#### Biais, Variance et Bruit

L'erreur irr√©cup√©rable est une partie de l'erreur qui ne peut pas √™tre r√©duite, car elle est caus√©e par le bruit des donn√©es.

```python
# Biais
Bias = np.mean(f_hat(x_i) - y_i)

# Variance
Variance = np.var(f_hat(x_i))

# Bruit
Bruit = "Erreur irr√©cup√©rable due au bruit des donn√©es"
```
- Le **biais** mesure l'√©cart entre la moyenne des pr√©dictions du mod√®le et la valeur que l'on essaie de pr√©dire.  
  Lorsque le biais est √©lev√©, le mod√®le fait souvent des pr√©dictions erron√©es.
  
- La **variance** d'un mod√®le fait r√©f√©rence √† sa sensibilit√© aux variations des donn√©es.  
  Si la variance est √©lev√©e, les pr√©dictions du mod√®le peuvent varier √©norm√©ment en fonction des donn√©es d'entra√Ænement.

## √âvaluation des Mod√®les de Classification

## 1. Matrice de confusion
|                | Pr√©dire Positif | Pr√©dire N√©gatif |
|---------------|----------------|----------------|
| **Classe positive** | TP | FN |
| **Classe n√©gative** | FP | TN |

## 2. Principales m√©triques
- **Taux de vrais positifs (Recall)** : \( TPR = \frac{TP}{P} \)
- **Taux de faux positifs** : \( FPR = \frac{FP}{N} \)
- **Taux de vrais n√©gatifs (Sp√©cificit√©)** : \( TNR = \frac{TN}{N} \)
- **Taux de faux n√©gatifs** : \( FNR = \frac{FN}{P} \)

## 3. Taux d'erreur pond√©r√©
\( \pi_P \times \text{FNR} + \pi_N \times \text{FPR} \)

## 4. Co√ªt de classification
\( C = C_{FP} \times FP + C_{FN} \times FN \)

üìå **Conclusion** : Comprendre ces m√©triques permet d'adapter les mod√®les aux besoins sp√©cifiques (sant√©, finance, s√©curit√©, etc.).

